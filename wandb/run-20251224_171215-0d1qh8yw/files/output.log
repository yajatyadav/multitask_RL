TODO(YY): Normalization path not implemented for libero_90-kitchen_scene2-open_the_top_drawer_of_the_cabinet
evaluation environment will return keys:  {'proprio', 'agentview_image', 'robot0_eye_in_hand_image', 'language'}
suites: ['libero_90'], scenes: ['kitchen_scene2']
All possible envs, sorted alphabetically: there are 1 total envs
 len(envs_to_eval)=6 Environments to evaluate: ['libero_spatial-pick_up_the_black_bowl_between_the_plate_and_the_ramekin_and_place_it_on_the_plate', 'libero_object-pick_up_the_alphabet_soup_and_place_it_in_the_basket', 'libero_goal-open_the_middle_drawer_of_the_cabinet', 'libero_90-KITCHEN_SCENE10_close_the_top_drawer_of_the_cabinet', 'libero_90-LIVING_ROOM_SCENE1_pick_up_the_alphabet_soup_and_put_it_in_the_basket', 'libero_90-STUDY_SCENE1_pick_up_the_book_and_place_it_in_the_front_compartment_of_the_caddy']
/home/yajatyadav/.local/share/uv/python/cpython-3.11.12-linux-x86_64-gnu/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
TODO(YY): Normalization path not implemented for libero_90-kitchen_scene2-open_the_top_drawer_of_the_cabinet
evaluation environment will return keys:  {'proprio', 'agentview_image', 'robot0_eye_in_hand_image', 'language'}
suites: ['libero_90'], scenes: ['kitchen_scene2']
All possible envs, sorted alphabetically: there are 1 total envs
 len(envs_to_eval)=6 Environments to evaluate: ['libero_spatial-pick_up_the_black_bowl_between_the_plate_and_the_ramekin_and_place_it_on_the_plate', 'libero_object-pick_up_the_alphabet_soup_and_place_it_in_the_basket', 'libero_goal-open_the_middle_drawer_of_the_cabinet', 'libero_90-KITCHEN_SCENE10_close_the_top_drawer_of_the_cabinet', 'libero_90-LIVING_ROOM_SCENE1_pick_up_the_alphabet_soup_and_put_it_in_the_basket', 'libero_90-STUDY_SCENE1_pick_up_the_book_and_place_it_in_the_front_compartment_of_the_caddy']
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
suites: ['libero_90'], scenes: ['kitchen_scene2']
ðŸ¤ªðŸ¤ªðŸ¤ª all_libero_env_names: {'libero_90': ['libero_90-KITCHEN_SCENE2_open_the_top_drawer_of_the_cabinet', 'libero_90-KITCHEN_SCENE2_put_the_black_bowl_at_the_back_on_the_plate', 'libero_90-KITCHEN_SCENE2_put_the_black_bowl_at_the_front_on_the_plate', 'libero_90-KITCHEN_SCENE2_put_the_middle_black_bowl_on_the_plate', 'libero_90-KITCHEN_SCENE2_put_the_middle_black_bowl_on_top_of_the_cabinet', 'libero_90-KITCHEN_SCENE2_stack_the_black_bowl_at_the_front_on_the_black_bowl_in_the_middle', 'libero_90-KITCHEN_SCENE2_stack_the_middle_black_bowl_on_the_back_black_bowl']}
ðŸ¤ªðŸ¤ªðŸ¤ª distinct_scenes: {'libero_90': ['KITCHEN_SCENE2']}
ðŸ˜ˆðŸ˜ˆðŸ˜ˆ suite='LIBERO_90' scene='KITCHEN_SCENE2' , there are 7 files for this suite + scene
ðŸ˜ˆðŸ˜ˆðŸ˜ˆ target_task_name='open_the_top_drawer_of_the_cabinet'
ðŸ¥³ðŸ¥³ðŸ¥³ j=0 Dataset open_the_top_drawer_of_the_cabinet has 3765, and flip_rewards=False, relabeled to target_task_name='open_the_top_drawer_of_the_cabinet'
the total size of the dataset is 3765
Evaluating agent with num_samples: 1
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Traceback (most recent call last):                                                                                                                                                                                                                                                                                                
  File "/home/yajatyadav/multitask_reinforcement_learning/multitask_RL/evaluation/eval_libero_best_of_N.py", line 160, in <module>                                                                                                                                                                                                
  File "/home/yajatyadav/multitask_reinforcement_learning/multitask_RL/evaluation/eval_libero_best_of_N.py", line 77, in sweep_best_of_N
    eval_info = eval_agent(agent, example_batch, names_to_return, logger, restore_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yajatyadav/multitask_reinforcement_learning/multitask_RL/evaluation/eval_libero_best_of_N.py", line 129, in eval_agent
    eval_info, trajs, renders = evaluate(
                                ^^^^^^^^^
  File "/home/yajatyadav/multitask_reinforcement_learning/multitask_RL/evaluation_libero.py", line 136, in evaluate
    next_observation, reward, done, info = env.step(np.clip(action, -1, 1)) # the done returned by env.step() already has 'or truncated' absorbed in
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yajatyadav/multitask_reinforcement_learning/multitask_RL/libero/libero/libero/envs/venv.py", line 794, in step
    env_return = self.workers[j].recv()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yajatyadav/multitask_reinforcement_learning/multitask_RL/libero/libero/libero/envs/venv.py", line 437, in recv
    result = self.parent_remote.recv()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yajatyadav/.local/share/uv/python/cpython-3.11.12-linux-x86_64-gnu/lib/python3.11/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/home/yajatyadav/.local/share/uv/python/cpython-3.11.12-linux-x86_64-gnu/lib/python3.11/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/home/yajatyadav/.local/share/uv/python/cpython-3.11.12-linux-x86_64-gnu/lib/python3.11/multiprocessing/connection.py", line 395, in _recv
    chunk = read(handle, remaining)
            ^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
